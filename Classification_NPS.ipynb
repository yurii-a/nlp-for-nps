{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDuZ1Bj95_Tg"
   },
   "source": [
    "# NPS classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('/Users/yurii/Downloads/nps.tsv',\n",
    "          mode = 'r',\n",
    "          encoding = 'ascii',\n",
    "          errors = 'ignore'\n",
    "         ) as csvfile:\n",
    "  nps_comments = pd.read_csv(csvfile, delimiter = '\\t')\n",
    "\n",
    "pt_comments = nps_comments.query(\"language == 'pt' and category\")[['category', 'comment']]\n",
    "pt_comments.columns = ['category', 'comment']\n",
    "pt_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Note: following nltk packages should be downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import (\n",
    "    sent_tokenize as splitter,\n",
    "    wordpunct_tokenize as tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = nltk.SnowballStemmer(u'portuguese')\n",
    "\n",
    "def stem(tokens_list):\n",
    "    return [stemmer.stem(token) for token in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits a string into sentences and words.\n",
    "def tokenize(text):\n",
    "  return [tokenizer(sentence) for sentence in splitter(text)]\n",
    "\n",
    "# In this exercise we do not care about the sentences (if any),\n",
    "# so let's flatten the list.\n",
    "def flatten(nested_list):\n",
    "  return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "def tokenize_flatten_df(row, field):\n",
    "  return flatten(tokenize(row[field]))\n",
    "\n",
    "import re\n",
    "\n",
    "# remove urls\n",
    "def remove_urls(text):\n",
    "  return re.sub(r\"(https?\\://)\\S+\", \"\", text)\n",
    "\n",
    "# remove mentions (@name) completely\n",
    "def remove_mentions(text):\n",
    "  return re.sub(r\"@[^:| ]+:? ?\", \"\", text)\n",
    "\n",
    "# remove \"RT:\", if the tweet contains it.\n",
    "def remove_rt(text):\n",
    "  if text.lower().startswith(\"rt:\"):\n",
    "    return text[3:].strip()\n",
    "  return text\n",
    "def remove_urls_mentions_rt_df(row, field):\n",
    "  return remove_rt(remove_mentions(remove_urls(row[field])))\n",
    "\n",
    "def replace_hashtags_from_text(text):\n",
    "  return re.sub(r\"#+ ?\", \"\", text)\n",
    "# remove hashtags\n",
    "def replace_hashtags_from_list(tokens_list):\n",
    "  return [token for token in tokens_list if token != \"#\"]\n",
    "\n",
    "# remove digits\n",
    "def remove_digits(tokens_list):\n",
    "  return [token for token in tokens_list \n",
    "                if not re.match(r\"[-+]?\\d+(\\.[0-9]*)?$\", token)]\n",
    "\n",
    "# remove all tokens that contains non alpha numeric, punctuation\n",
    "def remove_containing_non_alphanum(tokens_list):\n",
    "  return [token for token in tokens_list if token.isalpha()]\n",
    "# lowercase everything\n",
    "def lowercase_list(tokens_list):\n",
    "  return [token.lower() for token in tokens_list]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# remove stopwords\n",
    "def remove_stopwords(tokens_list):\n",
    "  return [token for token in tokens_list\n",
    "                if not token in stopwords.words(u'portuguese')]\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "def lemmatize(token):\n",
    "    return nlp(token)[0].lemma_\n",
    "\n",
    "def lemmatize_words(tokens_list):\n",
    "    return [lemmatize(token) for token in tokens_list]\n",
    "\n",
    "# Iterates over the elements of the list with tokens and performs cleanup.\n",
    "def clean_tokens(row, field):\n",
    "      return stem(\n",
    "          replace_hashtags_from_list(\n",
    "                remove_digits(\n",
    "                    remove_containing_non_alphanum(\n",
    "                        lowercase_list(\n",
    "                            remove_stopwords(\n",
    "                                lemmatize_words(row[field])))))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pt_comments['cleaned_comment'] = \\\n",
    "    pt_comments.apply(\n",
    "        lambda row: remove_urls_mentions_rt_df (row, 'comment'), axis=1)\n",
    "\n",
    "pt_comments['text_tokenized'] = \\\n",
    "    pt_comments.apply(\n",
    "        lambda row:\n",
    "            tokenize_flatten_df (row, 'comment'), axis=1)\n",
    "\n",
    "pt_comments['tokens'] = \\\n",
    "    pt_comments.apply(\n",
    "        lambda row:\n",
    "            clean_tokens (row, 'text_tokenized'), axis=1)\n",
    "\n",
    "pt_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BV5y4isKEfW1"
   },
   "source": [
    "## Running Naive Bayes classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the small groups under 'other' category\n",
    "print(pt_comments.groupby(['category']).count().query(\"comment > 100\"))\n",
    "dominant_labels = [\n",
    "    \"boleto-payin\",\n",
    "    \"borderless\",         \n",
    "    \"cost\",               \n",
    "    \"customer-support\",   \n",
    "    \"happy-customer\",     \n",
    "    \"interface\",          \n",
    "    \"limits\",             \n",
    "    \"payin-methods\",      \n",
    "    \"product-comms\",      \n",
    "    \"pt-content\",         \n",
    "    \"speed\",              \n",
    "    \"ted-payin\",          \n",
    "    \"verification\"]\n",
    "\n",
    "labels = []\n",
    "for label in pt_comments[\"category\"].values:\n",
    "    labels.append(label if label in dominant_labels else \"other_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating corpus of texts with corresponding golden labels.\n",
    "import numpy as np\n",
    "\n",
    "corpus = []\n",
    "for i, (document_id, row) in enumerate(pt_comments.iterrows()):\n",
    "  corpus.append(\" \".join(row['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyeS03Txv32i"
   },
   "outputs": [],
   "source": [
    "# We split the data into train/tet to avoid overfitting. Another strategy would be to do cross-validation, as below.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus_train, corpus_test, labels_train, labels_test = train_test_split(\n",
    "   corpus, labels, test_size=0.20, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOF7yi9EgHeD"
   },
   "outputs": [],
   "source": [
    "#@title Get feature representation of documents\n",
    "#    You can do it manually, just for fun, or we can already use some libs.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# We build document-term matrix for each dataset:\n",
    "document_term_matrix_train = vectorizer.fit_transform(corpus_train).toarray()\n",
    "document_term_matrix_test = vectorizer.transform(corpus_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsLmwnM_rVjp"
   },
   "outputs": [],
   "source": [
    "# Configuring evaluation function.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_classifier(classifier, X_train, X_test, y_train, y_test):\n",
    "  classifier.fit(X_train, y_train)\n",
    "  predicted_y_test = classifier.predict(X_test)\n",
    "  print(\"Accuracy:\", accuracy_score(y_test, predicted_y_test))\n",
    "  report = classification_report(y_test, predicted_y_test)\n",
    "  print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1563819302338,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "kLwc9JNxl3kB",
    "outputId": "251a17bc-0b70-48ee-8b0b-5ec92f8a2666",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Run evaluation with MultinomialNB:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = evaluate_classifier(MultinomialNB(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2939,
     "status": "ok",
     "timestamp": 1563819306394,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "tAMX3zZ5oeA1",
    "outputId": "fb281c76-b25d-40e0-8c24-534a547b5672"
   },
   "outputs": [],
   "source": [
    "#@title Run evaluation with Perceptron classifier:\n",
    "from sklearn.linear_model import Perceptron\n",
    "evaluate_classifier(Perceptron(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1549,
     "status": "ok",
     "timestamp": 1563819306626,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "F8Pr11jyo5G8",
    "outputId": "b8796f8f-5e77-4613-ef69-a19067713a12"
   },
   "outputs": [],
   "source": [
    "#@title Run evaluation with Logistic regression classifier:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "evaluate_classifier(LogisticRegression(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1563819308384,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "HPQvCifNt9MF",
    "outputId": "1e8b4e93-8839-4ea7-e8d2-49d2ea25d258"
   },
   "outputs": [],
   "source": [
    "#@title Run evaluation with Linear SVM classifier:\n",
    "from sklearn.svm import LinearSVC\n",
    "evaluate_classifier(LinearSVC(),\n",
    "                    document_term_matrix_train, document_term_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BX7B2LW2oVsA"
   },
   "source": [
    "Note: You can change the meta-parameters of classifiers, e.g. avoid using Prior for MNB, penalty for regulazation in LogRegression, etc.\n",
    "<br>The optimal meta-parameters are usually optimized on a separate tuning dataset using cross-validation\n",
    "<br>(see [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) in scikit-learn or [Vizier](http://go/vizier) for Google internal optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6FKVcc3O6nf"
   },
   "source": [
    "### Trying n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4356,
     "status": "ok",
     "timestamp": 1563819357114,
     "user": {
      "displayName": "Julia Proskurnia",
      "photoUrl": "https://lh4.googleusercontent.com/-Qke-0JLRMLo/AAAAAAAAAAI/AAAAAAAAIsM/X24zHBBNvWo/s64/photo.jpg",
      "userId": "11878887336903611948"
     },
     "user_tz": -120
    },
    "id": "5Q71IW4Cue8b",
    "outputId": "e036d157-4aba-4e44-d652-68cd2f4ad2cc"
   },
   "outputs": [],
   "source": [
    "# Check the results with n-grams.\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "document_ngram_matrix_train = ngram_vectorizer.fit_transform(corpus_train).toarray()\n",
    "document_ngram_matrix_test = ngram_vectorizer.transform(corpus_test).toarray()\n",
    "\n",
    "evaluate_classifier(LogisticRegression(), \n",
    "                    document_ngram_matrix_train, document_ngram_matrix_test,\n",
    "                    labels_train, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MDuZ1Bj95_Tg",
    "ctAAN8LC577n",
    "ZnCAcjjCgpWC",
    "1yIQpdKm9n7Q",
    "4PrjZyLUhArH",
    "kc5PfB3nh21k",
    "S21l8P2fh5qY",
    "WWCgpFuBhfHK",
    "V2ixTbyaYHdA",
    "_87XglopYOWB",
    "8esm8NmvhlRl",
    "BV5y4isKEfW1",
    "J6FKVcc3O6nf",
    "FztH9XpHj2Hl",
    "YJXXKnhlM6fK",
    "wRzww7KH0jZp",
    "hB7WabSt0n32",
    "BgLa9KfoP0Ia",
    "xrKoShoLRjU3",
    "oCt-eCyULukW",
    "bNCOH7GBSx4Q",
    "i6yQ0BjBwCqm",
    "HnjcHNZWv-iS",
    "3SfBdEQLo1eD",
    "vqhreqpeo4DM"
   ],
   "name": "Copy of 3.Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

